import os 
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import ReadTheDocsLoader 
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pprint import pprint 


load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY1")

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small", 
    api_key=OPENAI_API_KEY
    )
PINECONE_INDEX = "langchain-doc-index"


def ingest_docs():
    loader = ReadTheDocsLoader("D:/Genai_projects/Langchain - Develop LLM powered applications - Udemy Course/Documentation Helper/Documentation Scraping v3/langchain-docs/api.python.langchain.com/en/latest")

    raw_documents = loader.load()

    # for document in raw_documents:
    #     pprint(f"{document}\n---------------------\n")

    print(f"Loaded {len(raw_documents)} documents")
    """
    So, the tokens that we send to the LLM matter, every llm has a context limit. For eg, gpt-4o-mini has 1,28,000 
    context (https://platform.openai.com/docs/models/gpt-4o-mini) window (input + output tokens) and in that for output it has 
    16,384 token limit (Now this value will increase as cost of gpu's become cheaper). So, for the context that you are sending 
    to the LLM, if you decide that you will be sending 4 or 5 documents and you have allocated 2000 tokens for the context. Then 
    you have to chunk the documents into 500 words each (1 token can be a word, a set of words or a few alphabets: it varies). I 
    guess this is one way but it doesn't consider everything into, so how you chunk and how many chunks you need in your context is 
    totally dependent on the use case you are solving.
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)
    documents = text_splitter.split_documents(raw_documents)
    
    """
    Here, we are replacing the path of the langchain documentation with the link so that it can be easily clicked when navigation has to 
    happen from the text generated by the LLM the url originally was getting stored this way: D:\\... so, we are taking the exact text 
    after running a couple of times and getting to know the exact text with 
    
    for doc in documents:
        print(f"Raw metadata source: {repr(doc.metadata['source'])}")

    Because originally when it was being printed in the terminal it is being shown this way: D:\\\\Genai_projects...
    """
    for doc in documents:
        new_url = doc.metadata["source"]
        new_url = new_url.replace(
            "D:\\Genai_projects\\Langchain - Develop LLM powered applications - Udemy Course\\Documentation Helper\\Documentation Scraping v3\\langchain-docs", 
            "https:/")  
        new_url = new_url.replace("\\", "/")  # Convert backslashes to forward slashes
        doc.metadata.update({"source": new_url})
    


    for document in documents:
        pprint(f"{document}\n---------------------\n")
    
    print(f"Going to add {len(documents)} to Pinecone")


    PineconeVectorStore.from_documents(
        documents, embeddings, index_name=PINECONE_INDEX
    )

    print("*** Loading to Vector Store done ***")


def ingest_docs2() -> None:
    """
    This function is used to crawl the langchain website using firecrawl which would make the job easier for us removing 
    all the html tags and other unnecessary items and neatly structuring it into a markdown format very much preferred like how LLM likes
    it.
    """
    from langchain_community.document_loaders import FireCrawlLoader

    langchain_documents_base_urls = [
        "https://python.langchain.com/docs/integrations/chat/",
        "https://python.langchain.com/docs/integrations/retrievers/",
        "https://python.langchain.com/docs/integrations/tools/",
        "https://python.langchain.com/docs/integrations/document_loaders/",
        "https://python.langchain.com/docs/integrations/vectorstores/",
        "https://python.langchain.com/docs/integrations/text_embedding/",
        "https://python.langchain.com/docs/integrations/llms/",
        "https://python.langchain.com/docs/integrations/stores/",
        "https://python.langchain.com/docs/integrations/document_transformers/",
        "https://python.langchain.com/docs/integrations/llm_caching/",
        "https://python.langchain.com/docs/integrations/graphs/",
        "https://python.langchain.com/docs/integrations/memory/",
        "https://python.langchain.com/docs/integrations/callbacks/",
        "https://python.langchain.com/docs/integrations/chat_loaders/",
        "https://python.langchain.com/docs/integrations/adapters/",
        "https://python.langchain.com/docs/integrations/providers/",
        "https://python.langchain.com/docs/concepts/",
        "https://python.langchain.com/docs/how_to/",
        "https://python.langchain.com/docs/tutorials/",
        "https://python.langchain.com/docs/introduction/"
    ]

    for url in langchain_documents_base_urls:
        print(f"FireCrawling {url=}")
        loader = FireCrawlLoader(
            url=url,
            mode="crawl",
            params={
                "crawlerOptions": {"limit": 20},
                "pageOptions": {"onlyMainContent": True},
                "wait_until_done": True,
            },
            api_key=os.getenv("FIRECRAWL_API_KEY")
        )

        docs = loader.load()

        print(f"Going to add {len(docs)} documents to Pinecone!")
        PineconeVectorStore.from_documents(
            docs, embeddings, index_name="firecrawl-langchain-doc-index"
        )  # Here, we are not splitting it because the output we get from firecrawl is good. So, it is ok even if we directly store
        # them in the vector store
        print(f"******Loading {url}* to vectorstore done")


if __name__=="__main__":
    ingest_docs2()


