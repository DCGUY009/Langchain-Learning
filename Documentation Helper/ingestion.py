# Not able to make it work. Will look into this later as this is optional and is just to improve the RAG

import os 
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import ReadTheDocsLoader 
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pprint import pprint 
from typing import List
from firecrawl import FirecrawlApp
from langchain_community.document_loaders import FireCrawlLoader
import time

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY1")

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small", 
    api_key=OPENAI_API_KEY
    )
PINECONE_INDEX = "langchain-doc-index"


def ingest_docs():
    loader = ReadTheDocsLoader("D:/Genai_projects/Langchain - Develop LLM powered applications - Udemy Course/Documentation Helper/Documentation Scraping v3/langchain-docs/api.python.langchain.com/en/latest")

    raw_documents = loader.load()

    # for document in raw_documents:
    #     pprint(f"{document}\n---------------------\n")

    print(f"Loaded {len(raw_documents)} documents")
    """
    So, the tokens that we send to the LLM matter, every llm has a context limit. For eg, gpt-4o-mini has 1,28,000 
    context (https://platform.openai.com/docs/models/gpt-4o-mini) window (input + output tokens) and in that for output it has 
    16,384 token limit (Now this value will increase as cost of gpu's become cheaper). So, for the context that you are sending 
    to the LLM, if you decide that you will be sending 4 or 5 documents and you have allocated 2000 tokens for the context. Then 
    you have to chunk the documents into 500 words each (1 token can be a word, a set of words or a few alphabets: it varies). I 
    guess this is one way but it doesn't consider everything into, so how you chunk and how many chunks you need in your context is 
    totally dependent on the use case you are solving.
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)
    documents = text_splitter.split_documents(raw_documents)
    
    """
    Here, we are replacing the path of the langchain documentation with the link so that it can be easily clicked when navigation has to 
    happen from the text generated by the LLM the url originally was getting stored this way: D:\\... so, we are taking the exact text 
    after running a couple of times and getting to know the exact text with 
    
    for doc in documents:
        print(f"Raw metadata source: {repr(doc.metadata['source'])}")

    Because originally when it was being printed in the terminal it is being shown this way: D:\\\\Genai_projects...
    """
    for doc in documents:
        new_url = doc.metadata["source"]
        new_url = new_url.replace(
            "D:\\Genai_projects\\Langchain - Develop LLM powered applications - Udemy Course\\Documentation Helper\\Documentation Scraping v3\\langchain-docs", 
            "https:/")  
        new_url = new_url.replace("\\", "/")  # Convert backslashes to forward slashes
        doc.metadata.update({"source": new_url})
    


    for document in documents:
        pprint(f"{document}\n---------------------\n")
    
    print(f"Going to add {len(documents)} to Pinecone")


    PineconeVectorStore.from_documents(
        documents, embeddings, index_name=PINECONE_INDEX
    )

    print("*** Loading to Vector Store done ***")


def ingest_docs2(docs) -> None:
    if docs:
        print(f"Adding {len(docs)} documents into Pinecone...")

        for doc in docs:
            source_url = doc.metadata.get("sourceURL", "Unknown Source")  # Get URL from metadata
            print(f"Ingesting document from {source_url}...")

            PineconeVectorStore.from_documents([doc], embeddings, index_name="firecrawl-langchain-doc-index")

            print(f"Document from {source_url} loaded successfully into Pinecone.")
    else:
        print("No documents retrieved.")


def crawl_docs() -> List:
    """
    This function is used to crawl the langchain website using firecrawl which would make the job easier for us removing 
    all the html tags and other unnecessary items and neatly structuring it into a markdown format very much preferred like how LLM likes
    it.
    """

    langchain_documents_base_urls = [
        "https://python.langchain.com/docs/integrations/chat/",
        "https://python.langchain.com/docs/integrations/retrievers/",
        "https://python.langchain.com/docs/integrations/tools/",
        "https://python.langchain.com/docs/integrations/document_loaders/",
        "https://python.langchain.com/docs/integrations/vectorstores/",
        "https://python.langchain.com/docs/integrations/text_embedding/",
        "https://python.langchain.com/docs/integrations/llms/",
        "https://python.langchain.com/docs/integrations/stores/",
        "https://python.langchain.com/docs/integrations/document_transformers/",
        "https://python.langchain.com/docs/integrations/llm_caching/",
        "https://python.langchain.com/docs/integrations/graphs/",
        "https://python.langchain.com/docs/integrations/memory/",
        "https://python.langchain.com/docs/integrations/callbacks/",
        "https://python.langchain.com/docs/integrations/chat_loaders/",
        "https://python.langchain.com/docs/integrations/adapters/",
        "https://python.langchain.com/docs/integrations/providers/",
        "https://python.langchain.com/docs/concepts/",
        "https://python.langchain.com/docs/how_to/",
        "https://python.langchain.com/docs/tutorials/",
        "https://python.langchain.com/docs/introduction/"
    ]

    for url in langchain_documents_base_urls:
        print(f"FireCrawling {url=}")
        loader = FireCrawlLoader(
            url=url,
            mode="crawl",
            params={
                "limit": 20,
                "scrapeOptions": {
                    "onlyMainContent": True
                },
            },
            api_key=os.getenv("FIRECRAWL_API_KEY")
        )

        docs = loader.load()
        print(docs)
        return docs

        # print(f"Going to add {len(docs)} documents to Pinecone!")
        # PineconeVectorStore.from_documents(
        #     docs, embeddings, index_name="firecrawl-langchain-doc-index"
        # )  # Here, we are not splitting it because the output we get from firecrawl is good. So, it is ok even if we directly store
        # # them in the vector store
        # print(f"******Loading {url}* to vectorstore done")




def ingest_docs3():
    url = "https://python.langchain.com/docs/integrations/chat/"
    api_key = os.getenv("FIRECRAWL_API_KEY")

    if not api_key:
        print("Error: FIRECRAWL_API_KEY not set!")
        return

    app = FirecrawlApp(api_key=api_key)

    print(f"Starting FireCrawl for: {url}")

    # Start crawl
    crawl_response = app.crawl_url(url, params={"limit": 1, "scrapeOptions": {"onlyMainContent": True}})
    
    print("Crawl Response:", crawl_response)  # Debugging output

    job_id = crawl_response.get("jobId")
    if not job_id:
        print("Error: No job ID returned. Crawl might have failed.")
        return

    print(f"Job ID received: {job_id}")

    # Wait a few seconds before checking status
    time.sleep(10)

    # Check crawl status
    try:
        status_response = app.check_crawl_status(job_id)
        print("Crawl Status Response:", status_response)  # Debugging output

        status = status_response.get("status")
        if status == "completed":
            print("Crawl completed successfully!")
        elif status in ["failed", "stopped"]:
            print(f"Crawl {status}. Exiting.")
            return
        else:
            print(f"Crawl is still in progress: {status}")
    
    except Exception as e:
        print(f"Error checking crawl status: {e}")



if __name__=="__main__":
    # docs = crawl_docs()
    # ingest_docs2(docs)
    ingest_docs3()


